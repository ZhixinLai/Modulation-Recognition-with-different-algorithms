{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as vdatasets\nimport torchvision.utils as vutils\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport os, pickle\nimport h5py\nimport csv\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ntorch.manual_seed(1)\nnp.random.seed(1)\nrandom.seed(1) \ntorch.manual_seed(1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data and do FFT\n\ndef loaddata():\n    \n    f1 = h5py.File(\"/kaggle/input/modulation-prediction/data.hdf5\", 'r')\n    train_data_raw = np.array(f1['train'])\n    test_data_raw = np.array(f1['test'])\n\n    train_data_cmplx = train_data_raw[:,:,0] + 1j*train_data_raw[:,:,1]\n    test_data_cmplx = test_data_raw[:,:,0] + 1j*test_data_raw[:,:,1]\n    fft_train_data = np.fft.fft(train_data_cmplx,axis=1)\n    fft_test_data = np.fft.fft(test_data_cmplx,axis=1)\n\n    fft_train_data_real = np.real(fft_train_data)\n    fft_train_data_imag = np.imag(fft_train_data)\n    fft_test_data_real = np.real(fft_test_data)\n    fft_test_data_imag = np.imag(fft_test_data)\n\n\n    fft_train_data_complex = np.stack((fft_train_data_real,fft_train_data_imag),-1)\n    fft_test_data_complex = np.stack((fft_test_data_real,fft_test_data_imag),-1)\n\n    train_data = np.concatenate((train_data_raw, fft_train_data_complex), axis=2)\n    test_data = np.concatenate((test_data_raw, fft_test_data_complex), axis=2)\n\n    class_dic = { 'FM':0, 'OQPSK':1, 'BPSK':2, '8PSK':3, 'AM-SSB-SC':4, '4ASK':5, '16PSK':6, 'AM-DSB-SC':7, 'QPSK':8, 'OOK': 9 }\n    train_label = []\n\n    with open('/kaggle/input/modulation-prediction/train_labels.csv', newline='') as f2:\n        reader = csv.reader(f2)\n        for row in reader:\n            if row[1] in class_dic:\n                train_label.append(class_dic[row[1]])\n    train_label = np.array(train_label)\n    \n    train_data = np.float32(train_data).swapaxes(1,2)\n    test_data = np.float32(test_data).swapaxes(1,2)\n    return train_data, test_data, train_label\n\n\n# use FFT data do CNN doesn't have good performance\n# so I just use raw data rather than FFT data\n\ntrain_data, test_data, train_label = loaddata()\nX_train, X_val, y_train, y_val = train_test_split(train_data[:,:2,:], train_label, test_size=1/15, random_state=42)\nX_train = np.float32(X_train)\nX_val = np.float32(X_val)\nX_test = np.float32(test_data[:,:2,:])\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transfer data into torch format\n\ntransform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\nclass MyDataset(Dataset):\n    def __init__(self, data, label=None, transform=None):\n        datas = []\n        for i in range(data.shape[0]):\n            datas.append((data[i], int(label[i])))\n        self.datas = datas\n        self.transform = transform\n\n    def __getitem__(self, index):\n        fn, label = self.datas[index]\n        data = torch.from_numpy(fn)\n        return data,label\n\n    def __len__(self):\n        return len(self.datas)\n\ntrain = MyDataset(data = np.float32(X_train.reshape(-1, 2, 1024)), label = y_train)\ntrainloader = DataLoader(dataset=train, batch_size=32, shuffle=True)\n\nval = MyDataset(data = np.float32(X_val.reshape(-1, 2, 1024)), label = y_val)\nvalloader = DataLoader(dataset=val, batch_size=64, shuffle=True)\n\ny_test = np.zeros(X_test.shape[0])\ntest = MyDataset(data = np.float32(X_test[:,:].reshape(-1, 2, 1024)), label = y_test)\ntestloader = DataLoader(dataset=test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Neural Network\n\nclass FeedFrwdNet(nn.Module):\n    def __init__(self):\n        super(FeedFrwdNet,self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv1d(in_channels=2, out_channels=32, kernel_size=5, stride=1, padding=2), # output shape (32, 1024)\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2) # output shape (32, 512) \n        )\n        self.conv2 = nn.Sequential(  \n            nn.Conv1d(32, 32, 3, 1, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2)  # output shape (32, 256)\n        )\n        self.conv3 = nn.Sequential(  \n            nn.Conv1d(32, 32, 3, 1, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2)  # output shape (32, 128)\n        )\n        self.conv4 = nn.Sequential(  \n            nn.Conv1d(32, 32, 1, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2)  # output shape (32, 64)\n        )\n        self.conv5 = nn.Sequential(  \n            nn.Conv1d(32, 32, 1, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2)  # output shape (32, 32)\n        )\n        self.conv6 = nn.Sequential(  \n            nn.Conv1d(32, 32, 1, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2)  # output shape (32, 16)\n        )\n        self.conv7 = nn.Sequential(  \n            nn.Conv1d(32, 32, 1, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.AvgPool1d(kernel_size=2)  # output shape (32, 8)\n        )\n\n        # loss one\n        self.drop11 = nn.Dropout(0.5)\n        self.dense11 = nn.Sequential(\n            nn.Linear(32 * 8, 128),\n            nn.ReLU()\n        )\n        self.dense21 = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU()\n        )\n        self.dense31 = nn.Sequential(\n            nn.Linear(128, 10),\n        )\n\n        # loss two\n        self.drop12 = nn.Dropout(0.5)\n        self.dense12 = nn.Sequential(\n            nn.Linear(32 * 64, 128),\n            nn.ReLU()\n        )\n        self.dense22 = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU()\n        )\n        self.dense32 = nn.Sequential(\n            nn.Linear(128, 10),\n        )\n\n    def forward(self,input):\n        x1 = self.conv1(input)\n        x2 = self.conv2(x1)\n        x3 = self.conv3(x2)\n        x4 = self.conv4(x3)\n        x5 = self.conv5(x4)\n        x6 = self.conv6(x5)\n        x7 = self.conv7(x6)\n        \n        out1 = self.drop11(x7)\n        out1 = out1.view(out1.size(0), -1)\n        out1 = self.dense11(out1)\n        out1 = self.dense21(out1)\n        out1 = self.dense31(out1)\n\n        out2 = self.drop12(x4)\n        out2 = out2.view(out2.size(0), -1)\n        out2 = self.dense12(out2)\n        out2 = self.dense22(out2)\n        out2 = self.dense32(out2)\n\n        return out1, out2\n\n    \n# test function for train and val\n# store models with the top 3 score on val dateset\n\ndef test(acc_max1, acc_max2, acc_max3, loader, kind):\n    correct = 0\n    total = 0\n    for data in loader:\n        inputs, labels = data\n        model.eval()\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs[0].data * 1 + outputs[1].data * 0.5, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n    accuracy = 100.0 * correct / total\n    print(\"acc of \" + kind + \" is \"+ str(accuracy))\n    if accuracy > acc_max1:\n        acc_max1 = accuracy\n        print(\"acc_max1:\", acc_max1)\n        torch.save(model, 'model1.pkl')\n    elif accuracy <= acc_max1 and accuracy > acc_max2:\n        acc_max2 = accuracy\n        print(\"acc_max2:\", acc_max2)\n        torch.save(model, 'model2.pkl')\n    elif accuracy <= acc_max2 and accuracy > acc_max3:\n        acc_max3 = accuracy\n        print(\"acc_max3:\", acc_max3)\n        torch.save(model, 'model3.pkl')\n    return accuracy, acc_max1, acc_max2, acc_max3\n\n\n# define the model, loss and optimizer\n\nmodel = FeedFrwdNet()\nloss = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08, weight_decay=0.001) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the model\n\ndef train(epoch, log_interval=100):\n    for batch_idx, data in enumerate(trainloader,0):\n        inputs, labels = data\n        optimizer.zero_grad()\n        model.train()\n        output1, output2 = model(inputs)\n        loss_v = 1 * loss(output1, labels) + 0.5 * loss(output2, labels)\n        loss_v.backward()\n        optimizer.step()\n\nacc_60 = 0\nacc_80 = 0\nacc_100 = 0\nacc_120 = 0\nacc_140 = 0\nepochs= 140\nacc_max1 = 0\nacc_max2 = 0\nacc_max3 = 0\nmodel_save = FeedFrwdNet()\n\nfor epoch in range(1, epochs + 1):\n    if epoch < 60:\n        print(epoch)\n        train(epoch)\n        # test(trainloader, 'train')\n        tmp, acc_max1, acc_max2, acc_max3 = test(acc_max1, acc_max2, acc_max3, valloader, 'val')\n        acc_60 += tmp\n        \n    if epoch >= 60 and epoch <80: # print average score on val set for each stage\n        print(epoch)\n        train(epoch)\n        # test(trainloader, 'train')\n        tmp, acc_max1, acc_max2, acc_max3 = test(acc_max1, acc_max2, acc_max3, valloader, 'val')\n        acc_80 += tmp\n        \n    if epoch >= 80 and epoch <100:\n        print(epoch)\n        train(epoch)\n        # test(trainloader, 'train')\n        tmp, acc_max1, acc_max2, acc_max3 = test(acc_max1, acc_max2, acc_max3, valloader, 'val')\n        acc_100 += tmp\n        \n    if epoch >= 100 and epoch <120:\n        print(epoch)\n        train(epoch)\n        # test(trainloader, 'train')\n        tmp, acc_max1, acc_max2, acc_max3 = test(acc_max1, acc_max2, acc_max3, valloader, 'val')\n        acc_120 += tmp\n        \n    if epoch >= 120 and epoch <140:\n        print(epoch)\n        train(epoch)\n        # test(trainloader, 'train')\n        tmp, acc_max1, acc_max2, acc_max3 = test(acc_max1, acc_max2, acc_max3, valloader, 'val')\n        acc_140 += tmp\n\nprint(acc_60/60)  \nprint(acc_80/20)\nprint(acc_100/20)\nprint(acc_120/20)\nprint(acc_140/20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in the previous training process, we stored the top 3 model\n# here, load one of the three models\n# because the third top model achieve the best score on private board\n# we just use this model model3.pkl \n# if you don't want to wait for more than 2 hours to train the model\n# you can directly use the model I attached at the Zip file\n# which is the same as here\n\ndef test(loader):\n    correct = 0\n    total = 0\n    model = FeedFrwdNet()\n    model = torch.load('model3.pkl')\n    result = []\n    for data in loader:\n        inputs, labels = data\n        model.eval()\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs[0].data * 1 + outputs[1].data * 0.5, 1)\n        result.extend(predicted.numpy().tolist())\n    return result\n\ntest_result = test(testloader) # result of test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# store result in csv file\n\ntest_pre = test_result\nrows_write = []\nname_dic = { 0:'FM', 1:'OQPSK', 2:'BPSK', 3:'8PSK', 4:'AM-SSB-SC', 5:'4ASK', 6:'16PSK', 7:'AM-DSB-SC', 8:'QPSK', 9:'OOK' }\nfor i in range(len(test_pre)):\n    rows_write.append(name_dic[test_pre[i]])\nsubmit_df = pd.DataFrame({\"Id\": range(len(test_pre)), \"Category\": rows_write})\nsubmit_df.to_csv(\"submission3.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}