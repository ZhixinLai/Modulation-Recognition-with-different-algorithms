{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle  \nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nimport os,random\nimport sys, math, cmath\nfrom time import time\nfrom IPython.display import Markdown, display\nfrom collections import defaultdict","execution_count":5,"outputs":[{"output_type":"stream","text":"/kaggle/input/modulation-prediction/data.hdf5\n/kaggle/input/modulation-prediction/train_labels.csv\n/kaggle/input/modulation-prediction/classes.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport h5py\nimport csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ndef loaddata():\n    \n    f1 = h5py.File(\"/kaggle/input/modulation-prediction/data.hdf5\", 'r')\n    train_data_raw = np.array(f1['train'])\n    test_data_raw = np.array(f1['test'])\n\n    train_data_cmplx = train_data_raw[:,:,0] + 1j*train_data_raw[:,:,1]\n    test_data_cmplx = test_data_raw[:,:,0] + 1j*test_data_raw[:,:,1]\n    fft_train_data = np.fft.fft(train_data_cmplx,axis=1)\n    fft_test_data = np.fft.fft(test_data_cmplx,axis=1)\n\n    fft_train_data_real = np.real(fft_train_data)\n    fft_train_data_imag = np.imag(fft_train_data)\n    fft_test_data_real = np.real(fft_test_data)\n    fft_test_data_imag = np.imag(fft_test_data)\n\n\n    fft_train_data_complex = np.stack((fft_train_data_real,fft_train_data_imag),-1)\n    fft_test_data_complex = np.stack((fft_test_data_real,fft_test_data_imag),-1)\n\n    train_data = np.concatenate((train_data_raw, fft_train_data_complex), axis=2)\n    test_data = np.concatenate((test_data_raw, fft_test_data_complex), axis=2)\n\n\n    class_dic = { 'FM':0, 'OQPSK':1, 'BPSK':2, '8PSK':3, 'AM-SSB-SC':4, '4ASK':5, '16PSK':6, 'AM-DSB-SC':7, 'QPSK':8, 'OOK': 9 }\n    train_label = []\n\n    with open('/kaggle/input/modulation-prediction/train_labels.csv', newline='') as f2:\n        reader = csv.reader(f2)\n        for row in reader:\n            if row[1] in class_dic:\n                train_label.append(class_dic[row[1]])\n    train_label = np.array(train_label)\n    \n    train_data = np.float32(train_data).swapaxes(1,2)\n    test_data = np.float32(test_data).swapaxes(1,2)\n    return train_data, test_data, train_label\n\nX_train, X_test, y_train_all = loaddata()\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train_all.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"(30000, 4, 1024)\n(20000, 4, 1024)\n(30000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature function\n\n# part one: basic feature \n\ndef element_mean(x):\n    return np.mean(x, axis=2, keepdims=True)\n\ndef element_abs_mean(x):\n    return np.abs(np.mean(x, axis=2, keepdims=True))\n\ndef element_std(x):\n    return np.std(x,axis=2,keepdims=True)\n\ndef element_std_abs(x):\n    return np.std(np.abs(x),axis=2,keepdims=True)  \n\ndef abs_max_mins_min(x):\n    return np.abs(np.max(x, axis=2, keepdims=True) - np.min(x, axis=2, keepdims=True))\n\ndef max_mins_min_abs(x):\n    return np.max(np.abs(x), axis=2, keepdims=True) - np.min(np.abs(x), axis=2, keepdims=True)\n\ndef abs_mean_sqr(x):\n    return np.abs(np.mean(np.square(x), axis = 2, keepdims=True))\n\ndef mean_sqr_abs(x):\n    return np.mean(np.square(np.abs(x)), axis = 2, keepdims=True)\n\ndef abs_sqrmean_mis_meansqr(x):\n    return np.abs(np.mean(np.square(x),axis=2,keepdims=True)- \\\n            np.square(np.mean(x,axis = 2,keepdims=True)))\n    \ndef sqrmean_mis_meansqr_abs(x):\n    return np.mean(np.square(np.abs(x)),axis=2,keepdims=True)- \\\n            np.square(np.mean(np.abs(x),axis = 2,keepdims=True))\n\ndef abs_fourmean_mis_meanfour(x):\n    return np.abs(np.mean(np.power(x,4),axis=2,keepdims=True)-\\\n            np.mean(np.square(x),axis=2,keepdims=True)**2) \n\ndef fourmean_mis_meanfour_abs(x):\n    return np.mean(np.power(np.abs(x),4),axis=2,keepdims=True)-\\\n            np.mean(np.square(np.abs(x)),axis=2,keepdims=True)**2\n\n\n# part two: time sequence feature\n\ndef absolute_sum_of_changes(x):\n    return np.sum(np.abs(np.diff(x,1)), axis=2, keepdims=True)\n\ndef agg_autocorrelation(x, l=1):    ##\n    x_var = np.var(x, axis=2, keepdims=True)\n    x_diff_sum = np.sum(np.diff(x-np.mean(x, axis=2, keepdims=True),l), axis=2, keepdims=True)\n    return x_diff_sum / x_var\n\ndef approximate_entropy(x):\n    return np.sum(-x*np.log(np.abs(x+0.0000001)), axis=2, keepdims=True)\n\ndef autocorrelation(x, k=1):\n    n = x.shape[2]\n    variance = np.var(x, axis=2, keepdims=True)+0.0000001\n    x = x-np.mean(x, axis=2, keepdims=True)\n    result = np.sum(x[:,:,k:]*x[:,:,:n-k], axis=2, keepdims=True) / (variance*(n-k))\n    return result\n\ndef cwt_coefficients(x):\n    ricker = (1-x**2/4)*np.exp(-x**2/8)\n    result = np.mean(ricker, axis=2, keepdims=True)\n    return result\n\ndef count_above_mean(x):\n    mask = (x - np.mean(x, axis=2, keepdims=True))>0\n    result = np.sum(mask, axis=2, keepdims=True)\n    return result\n\ndef count_below_mean(x):\n    mask = (x - np.mean(x, axis=2, keepdims=True))<0\n    result = np.sum(mask, axis=2, keepdims=True)\n    return result\n\ndef first_location_of_maximum(x): ##\n    result = np.argmax(x, axis=2)\n    return result.reshape(result.shape[0], result.shape[1], 1)\n\ndef first_location_of_minimum(x): ##\n    result = np.argmin(x, axis=2)\n    return result.reshape(result.shape[0], result.shape[1], 1)\n\ndef kurtosis(x):\n    x = x - np.mean(x, axis=2, keepdims=True)\n    e4 = np.mean(pow(x, 4), axis=2, keepdims=True)\n    e2 = pow(np.mean(pow(x, 2), axis=2, keepdims=True),2)\n    result = e4/e2\n    return result\n\ndef skewness(x):\n    variance = np.var(x, axis=2, keepdims=True)+0.0000001\n    x = x - np.mean(x, axis=2, keepdims=True)\n    result = np.mean(pow(x/variance, 3), axis=2, keepdims=True)\n    return result\n\n# part three: modulation feature\n\ndef max_amp_density_abs(X_complex_abs):\n    \n    # The maximum value of the normalized spectral density of the amplitude with Zero Center\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    Acn_complex_max = np.max(np.abs(np.fft.fft(Acn,axis=2))**2, axis = 2, keepdims = True) / X_complex_abs.shape[2]\n    return Acn_complex_max\n    \ndef std_phase_abs(X_complex_angl, X_complex_abs):\n    # Standard deviation of absolute value of instantaneous phase nonlinear component at zero Center\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    sqr_mask = Acn > 0.05\n    sqr_mask[:,:,0] = True\n    sqr_mask_sum = np.sum(sqr_mask,axis=2,keepdims=True)\n    angl_mins_mean = X_complex_angl - np.mean(X_complex_angl,axis=2,keepdims=True)\n    angl_mins_mean_after_mask = angl_mins_mean * sqr_mask\n    result = np.sqrt(np.sum(np.square(angl_mins_mean_after_mask),axis=2,keepdims=True)/sqr_mask_sum -\n                                     (np.sum(np.abs(angl_mins_mean_after_mask),axis=2,keepdims=True)/sqr_mask_sum)**2)\n    return result\n\ndef std_phase(X_complex_angl, X_complex_abs):\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    sqr_mask = Acn > 0.05\n    sqr_mask[:,:,0] = True\n    sqr_mask_sum = np.sum(sqr_mask,axis=2,keepdims=True)\n    angl_mins_mean = X_complex_angl - np.mean(X_complex_angl,axis=2,keepdims=True)\n    angl_mins_mean_after_mask = angl_mins_mean * sqr_mask\n    result = np.sqrt(np.sum(np.square(angl_mins_mean_after_mask),axis=2,keepdims=True)/sqr_mask_sum - \n                                     (np.sum(angl_mins_mean_after_mask,axis=2,keepdims=True)/sqr_mask_sum)**2) \n    return result\n\ndef std_amp_abs(X_complex_abs):\n    \n    # Standard deviation of instantaneous amplitude absolute value of Zero Center normalized non weak signal\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    result = np.sqrt(np.mean(np.square(Acn),axis=2,keepdims=True) - \n                                     np.mean(np.abs(Acn),axis=2,keepdims=True)**2) \n    return result\n\ndef std_fre_abs(X_complex_fre, X_complex_abs):\n    # Standard deviation of instantaneous frequency absolute value of Zero Center normalized non weak signal\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    sqr_mask = Acn > 0.05\n    sqr_mask[:,:,0] = True\n    sqr_mask_sum = np.sum(sqr_mask,axis=2,keepdims=True)\n    fn = X_complex_fre - np.mean(X_complex_fre,axis=2,keepdims=True)\n    fn_after_mask = fn * sqr_mask\n    result = np.sqrt(np.sum(np.square(fn_after_mask),axis=2,keepdims=True)/sqr_mask_sum - \n                                     (np.sum(np.abs(fn_after_mask),axis=2,keepdims=True)/sqr_mask_sum)**2) \n    return result\n\ndef std_amp(X_complex_abs):\n    # Standard deviation of instantaneous amplitude of Zero Center normalized non weak signal\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    sqr_mask = Acn > 0.05\n    sqr_mask[:,:,0] = True\n    sqr_mask_sum = np.sum(sqr_mask,axis=2,keepdims=True)\n    Acn_after_mask = Acn * sqr_mask\n    result = np.sqrt(np.sum(np.square(Acn_after_mask),axis=2,keepdims=True)/sqr_mask_sum - \n                                     (np.sum(Acn_after_mask,axis=2,keepdims=True)/sqr_mask_sum)**2) \n    return result    \n\ndef den_amp(X_complex_abs):\n    # Compactness of Zero Center normalized instantaneous amplitude\n    Acn = X_complex_abs / np.mean(X_complex_abs, axis=2, keepdims = True) - 1\n    result = np.mean(np.power(Acn, 4),axis=2,keepdims=True) / np.mean(np.square(Acn),axis=2,keepdims=True)**2 \n    return result \n\ndef den_fre(X_complex_fre):\n    # Compactness of Zero Center normalized instantaneous frequency\n    fn = X_complex_fre - np.mean(X_complex_fre,axis=2,keepdims=True)\n    result = np.mean(np.power(fn, 4),axis=2,keepdims=True) / np.mean(np.square(fn),axis=2,keepdims=True)**2 \n    return result \n \n# def accumulat_features(x): # accumulation feature and the input is complex\n    \n#     x_r = np.conjugate(x)\n\n#     M20 = np.mean(np.power(x, 2), axis = 2, keepdims = True)\n#     M21 = np.mean(np.power(np.abs(x), 2), axis = 2, keepdims = True)\n#     M22 = np.mean(np.power(x_r, 2), axis = 2, keepdims = True)\n#     M40 = np.mean(np.power(x, 4), axis = 2, keepdims = True)\n#     M41 = np.mean(np.power(np.abs(x), 2)*np.power(x, 2), axis = 2, keepdims = True)\n#     M42 = np.mean(np.power(np.abs(x), 4), axis = 2, keepdims = True)\n#     M43 = np.mean(np.power(np.abs(x), 2)*np.power(x_r, 2), axis = 2, keepdims = True)\n#     M60 = np.mean(np.power(x, 6), axis = 2, keepdims = True)\n#     M63 = np.mean(np.power(np.abs(x), 6), axis = 2, keepdims = True)\n#     M80 = np.mean(np.power(x, 8), axis = 2, keepdims = True)\n\n#     C20 = M20\n#     C21 = M21\n#     C40 = M40 - 3*np.power(M20, 2)\n#     C41 = M41 - 3*M20*M21\n#     C42 = M42 - np.power(np.abs(M20), 2) - 2*np.power(M21, 2)\n#     C60 = M60 - 15*M20*M40 + 30*np.power(M20, 3)\n#     C63 = M63 - 6*M41*M20 - 9*M42*M21 + 18*M20*M20*M21 + 12*np.power(M21, 3)\n#     C80 = M80 - 28*M20*M60 - 35*M40*M40 + 420*M20*M20*M40 - 630*np.power(M20, 4)\n\n#     T1 = np.abs(C80) / np.power(np.abs(C42), 2)\n#     T2 = np.abs(C80) / np.power(np.abs(C40), 2) # 利用差分（x(k+1) - x(k)）可区分QPSK/OQPSK\n#     M1 = np.abs(C40) / np.abs(C42)\n#     M2 = np.power(np.abs(C63), 2) / np.power(np.abs(C42), 3)\n\n#     C20_norm = np.abs(C20/C21)\n#     C40_norm = np.abs(C40/np.power(C21, 2))\n#     C41_norm = np.abs(C41/np.power(C21, 2))\n#     C42_norm = np.abs(C42/np.power(C21, 2))\n#     C60_norm = np.abs(C60/np.power(C21, 3))\n#     C63_norm = np.abs(C63/np.power(C21, 3))\n#     C80_norm = np.abs(C80/np.power(C21, 4))\n\n#     return [T1, T2, M1, M2, C20_norm, C40_norm, C41_norm, C42_norm, C60_norm, C63_norm, C80_norm]    \n    ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature filter with the index got from random forest\n\nindex_layer_list = ['c 0 0 3', 't 5 0 20', 't 5 1 20', 'c 0 0 4', 't 3 0 8', 't 3 1 2', 't 6 1 20', 'c 0 3 0', 't 3 0 20', 't 3 3 3', 't 5 1 13', 'c 0 2 3', 't 5 3 8', 'c 1 0 1', 't 4 1 2', 'c 1 1 5', 'c 1 2 5', 't 6 1 19', 't 3 2 9', 't 3 0 2', 'c 0 1 3', 'c 0 1 2', 't 3 1 9', 't 5 3 25', 't 3 3 25', 'c 1 3 5', 't 6 2 19', 't 6 0 20', 'c 0 1 0', 't 4 1 11', 'c 0 0 2', 't 4 0 20', 't 3 2 20', 't 2 0 23', 't 4 1 3', 't 3 4 20', 't 4 4 13', 't 2 1 6', 't 3 4 0', 't 7 1 18', 't 5 4 17', 't 3 1 20', 'c 0 2 2', 't 3 3 9', 't 0 4 23', 't 2 3 23', 't 7 1 26', 't 3 2 2', 't 5 1 6', 't 7 1 27', 't 6 2 2', 't 3 3 20', 't 4 4 7', 't 6 3 23', 't 5 0 9', 't 2 0 25', 't 3 4 8', 't 3 4 2', 't 6 3 10', 't 0 0 24', 't 6 2 3', 't 4 2 2', 't 4 0 22', 't 3 2 8', 'c 1 2 4', 't 0 4 21', 't 7 0 25', 't 0 4 24', 't 3 4 9', 't 5 4 22', 't 5 4 9', 't 5 3 3', 't 8 0 24', 't 6 3 21', 't 4 1 15', 't 5 3 5', 't 3 4 24', 't 5 2 21', 't 3 3 5', 't 5 3 11', 't 6 4 23', 't 6 3 9', 't 5 4 25', 't 4 3 4', 't 3 0 18', 't 3 4 23', 't 6 4 16', 't 4 1 9', 't 6 3 3', 't 6 2 10', 't 3 0 16', 't 3 0 26', 't 3 0 3', 't 5 1 15', 't 4 0 2', 't 6 1 16', 't 6 1 15', 't 5 3 24', 't 4 1 8', 't 4 0 15', 't 5 3 21', 't 6 2 14', 't 5 0 15', 't 5 2 14', 'c 1 0 4', 't 3 0 15', 'c 0 3 3', 't 0 1 14', 'c 0 2 0', 't 5 0 16', 't 4 1 6', 't 3 1 8', 't 3 3 10', 't 3 3 4', 'c 0 3 2', 't 2 1 17', 't 3 1 19', 't 6 3 26', 't 3 1 7', 't 5 3 27', 't 3 1 16', 't 3 2 3', 't 6 3 27', 't 1 2 14', 'c 1 4 5', 't 5 3 4', 't 6 1 1', 't 4 1 16', 't 6 0 15', 't 5 0 14', 't 7 1 21', 't 2 3 14', 't 4 2 9', 't 3 3 8', 't 6 3 11', 't 5 1 1', 't 4 1 14', 't 3 0 27', 't 6 2 0', 't 3 4 14', 'c 0 2 5', 't 3 0 9', 't 6 3 5', 't 4 4 23', 't 4 0 8', 't 2 2 24', 't 3 4 21', 't 4 0 9', 't 3 3 14', 't 5 1 9', 't 5 3 10', 't 6 3 25', 't 2 1 3', 't 3 4 27', 't 4 0 18', 't 2 2 9', 't 5 4 14', 't 6 0 16', 't 3 3 2', 't 6 4 14', 't 5 4 23', 't 4 2 14', 't 3 1 3', 't 5 1 16', 't 4 0 3', 't 4 1 23', 'c 0 4 2', 't 5 3 14', 't 3 0 14', 't 8 4 26', 't 2 2 22', 't 0 2 15', 't 6 4 15', 't 2 2 8', 't 4 3 14', 't 7 1 8', 't 6 0 14', 't 4 1 19', 't 4 0 14', 't 3 1 14', 't 2 3 11', 't 4 3 5', 't 8 1 10', 't 2 2 16', 't 0 3 14', 't 5 4 18', 't 1 0 12', 't 7 1 9', 't 3 4 1', 't 3 3 18', 't 2 2 14', 't 6 4 20', 't 6 3 4', 't 7 1 10', 't 2 3 7', 't 3 4 25', 't 3 2 10', 't 4 0 16', 'c 0 1 4', 't 4 0 21', 't 7 1 23', 't 2 2 10', 't 5 4 15', 't 3 3 11', 't 2 0 8', 't 6 3 14', 't 6 1 14', 't 5 4 21', 't 2 0 3', 't 2 1 0', 't 1 1 6', 't 7 3 14', 't 4 0 26', 't 8 0 26', 't 6 4 18', 't 3 1 23', 't 2 1 11', 't 4 2 11', 't 3 4 26', 't 3 0 10', 't 3 0 0', 't 6 1 2', 't 7 1 22', 't 8 1 26', 't 5 4 12', 't 2 2 1', 't 7 1 24', 't 7 4 12', 't 2 1 9', 't 4 2 27', 't 5 2 8', 't 3 1 15', 't 2 1 2', 't 2 2 3', 't 0 3 4', 't 5 3 26', 't 4 2 24', 't 2 1 14', 'c 0 0 5', 'c 1 0 5', 't 5 1 14', 't 5 4 27', 't 4 1 7', 't 5 0 12', 't 3 2 27', 't 6 4 21', 't 6 1 7', 't 6 3 18', 't 2 0 1', 't 6 0 23', 't 4 2 7', 't 5 0 3', 't 0 4 16', 't 2 0 6', 't 2 0 11', 't 2 3 9', 't 4 1 21', 't 2 1 16', 't 5 3 20', 't 5 1 19', 't 8 3 27', 't 0 0 17', 't 2 2 7', 't 4 4 16', 't 2 2 2', 't 5 1 7', 't 7 1 14', 't 3 3 0', 't 1 2 8', 't 3 4 10', 't 0 2 8', 't 0 1 13', 't 4 2 19', 't 1 1 13', 't 1 1 15', 't 4 4 12', 't 3 0 25', 't 0 4 25', 't 4 0 27', 't 7 0 12', 't 2 4 23', 't 6 1 22', 't 8 2 18', 't 5 2 19', 't 3 0 24', 't 7 1 1', 't 5 0 18', 't 5 1 17', 't 6 1 13', 't 2 1 24', 't 6 3 19', 'c 0 3 1', 't 3 1 22', 't 3 3 27', 't 4 4 14', 't 1 4 14', 't 0 4 12', 't 2 1 1', 't 3 2 16', 't 5 1 21', 't 1 0 18', 't 4 0 24', 't 8 2 27', 'c 0 4 5', 't 4 0 12', 't 8 4 27', 't 2 0 10', 't 2 3 17', 't 2 0 9', 'c 0 1 5', 't 4 1 10', 't 5 2 25', 't 4 3 11', 't 5 3 17', 't 6 1 8', 't 4 2 12', 'c 1 0 2', 't 4 2 8', 't 7 2 21', 't 5 3 23', 't 6 4 11', 't 4 3 27', 't 3 2 14', 't 5 2 13', 't 4 0 23', 't 2 1 13', 't 3 4 3', 't 5 4 20', 't 1 1 14', 't 5 0 2', 't 2 1 23', 'c 1 4 4', 't 2 4 14', 't 3 3 12', 't 2 2 5', 't 4 0 0', 't 3 1 24', 't 1 1 23', 't 0 4 14', 't 7 1 2', 't 2 2 13', 't 3 3 26', 't 6 4 12', 't 4 1 17', 't 5 3 18', 't 6 2 20', 't 5 1 23', 't 1 2 13', 't 0 4 5', 't 0 0 8', 't 6 1 3', 't 3 3 23', 't 6 4 5', 't 4 1 24', 't 1 3 10', 't 4 0 19', 't 7 2 16', 't 1 3 23', 't 1 4 12', 't 1 2 12', 't 4 0 10', 't 0 1 27', 't 1 4 13', 't 8 0 27', 't 3 2 22', 't 0 1 22', 't 1 2 24', 't 2 2 21', 't 8 2 21', 't 5 4 16', 't 5 2 26', 't 4 4 25', 't 7 1 17', 't 3 0 17', 't 0 2 17', 't 4 2 22', 'c 1 0 3', 't 6 4 9', 't 8 1 18', 't 4 3 19', 't 8 1 8', 'c 1 1 3', 'c 1 3 4', 't 5 2 9', 't 1 3 4', 't 6 0 10', 't 4 3 25', 't 1 2 0', 't 6 2 16', 't 0 1 15', 't 1 3 27', 't 8 4 18', 't 4 3 26', 't 1 0 24', 't 5 0 1', 't 6 0 12', 'c 1 2 1', 't 2 3 2', 't 2 0 12', 't 6 0 5', 't 5 1 3', 't 0 1 16', 't 2 2 11', 't 2 4 24', 't 6 2 15', 't 2 0 0', 't 1 0 22', 't 8 0 12', 't 6 0 0', 't 6 2 9', 't 2 4 9', 't 6 4 10', 't 8 0 7', 't 3 4 15', 't 8 0 17', 't 6 0 19', 't 6 2 26', 't 2 1 8', 't 4 4 27', 't 4 1 22', 't 7 2 24', 't 1 4 16', 'c 0 2 4', 't 0 2 14', 't 6 4 24', 't 7 3 25', 't 6 0 1', 't 1 4 0', 't 4 4 8', 't 2 0 14', 't 1 0 2', 't 4 2 16', 't 1 4 1', 't 1 2 17', 'c 1 4 1', 't 4 4 4', 't 5 1 10', 't 6 3 1', 't 5 3 13', 't 8 0 6', 't 3 4 18', 't 6 4 8', 't 0 3 25', 't 1 0 10', 't 2 4 12', 't 0 0 27', 't 4 2 6', 't 0 2 11', 't 3 2 25', 't 5 2 22', 't 1 3 18', 't 4 2 1', 't 1 4 26', 't 6 3 2', 't 6 0 3', 't 8 2 2', 't 6 0 21', 't 0 2 26', 't 1 2 16', 't 4 1 12', 't 4 4 6', 't 5 3 7', 't 6 1 27', 't 6 2 13', 't 1 0 7', 't 6 0 9', 't 0 1 0', 't 3 1 21', 't 0 3 5', 't 0 0 12', 't 0 3 27', 't 2 1 7', 't 8 4 19', 't 7 1 12', 'c 0 4 4', 't 4 3 24', 't 0 3 18', 't 3 1 18', 't 6 2 6', 't 2 2 25', 't 2 1 19', 't 3 0 23', 't 3 1 25', 't 5 0 21', 'c 0 3 4', 't 2 3 22', 't 5 3 15', 't 2 1 12', 't 5 2 3', 't 3 2 15', 't 6 4 22', 't 3 2 11', 't 8 2 12', 't 6 3 22', 't 8 4 21', 't 6 1 11', 't 0 1 8', 't 4 4 5', 't 2 1 25', 't 0 0 26', 'c 0 4 0', 't 6 0 8', 't 3 4 12', 't 2 4 6', 't 2 0 17', 't 2 2 6', 't 6 4 4', 't 4 1 18', 't 3 2 4', 't 7 2 11', 't 0 2 5', 'c 1 1 4', 't 6 1 23', 't 0 0 5', 't 4 0 1', 't 2 0 24', 't 7 4 5', 't 5 4 4', 't 7 4 16', 'c 1 0 0', 't 4 2 5', 't 5 3 1', 't 4 4 26', 't 0 1 5', 't 0 0 14', 't 8 3 21', 't 1 3 16', 't 4 3 22', 't 8 0 8', 't 1 1 10', 't 0 2 24', 't 4 3 0', 't 2 3 5', 't 5 3 9', 't 6 4 26', 't 1 0 21', 't 6 4 25', 't 1 2 15', 't 5 4 26', 't 4 2 21', 't 3 1 5', 't 3 1 10', 't 6 2 24', 't 7 1 16', 't 8 1 21', 't 0 0 18', 't 8 4 9', 't 1 0 14', 't 7 0 14', 't 6 3 12', 't 8 4 3', 't 3 0 7', 't 2 3 3', 't 1 4 15', 't 5 4 10', 't 6 2 22', 't 6 4 27', 't 8 4 14', 't 7 4 0', 't 3 2 19', 't 5 2 23', 't 8 4 2', 't 0 1 25', 't 5 4 0', 't 0 4 4', 't 7 0 2', 't 3 1 11', 'c 1 3 1', 't 1 2 1', 't 7 4 25', 't 8 4 24', 't 7 0 15', 't 4 4 15', 't 5 4 11', 't 1 1 0', 't 3 2 12', 't 8 3 18', 't 8 1 23', 't 3 0 11', 't 0 4 9', 't 1 4 2', 't 5 0 25', 't 7 1 25', 't 7 4 14', 'c 1 4 2', 't 4 3 10', 't 4 3 3', 't 1 4 4', 't 0 4 0', 't 2 4 22', 't 4 3 20', 't 3 2 24', 't 4 1 25', 't 5 1 0', 'c 1 1 2', 't 8 1 5', 't 3 4 5', 't 1 3 0', 't 6 3 8', 't 5 1 8', 't 8 3 24', 't 5 4 5', 't 5 3 12', 't 2 4 8', 't 8 1 4', 't 7 2 23', 't 7 3 1', 'c 0 3 5', 't 3 2 17', 't 0 2 0', 't 8 0 0', 't 0 4 2', 't 3 4 19', 't 6 1 6', 't 3 3 21', 't 8 4 11', 't 2 0 19', 't 8 2 14', 't 4 3 23', 't 5 2 16', 't 5 2 15', 't 0 0 22', 't 8 2 23', 't 2 3 8', 't 7 3 15', 't 0 3 10', 't 7 2 19', 't 7 3 5', 't 2 4 21', 't 2 0 18', 't 3 2 18', 't 1 2 26', 't 8 2 19', 't 0 4 27', 't 5 2 2', 't 6 3 20', 't 4 1 0', 't 4 2 3', 't 1 2 22', 't 0 3 22', 't 0 0 1', 'c 0 0 0', 't 0 4 26', 't 0 3 2', 't 5 4 24', 't 5 2 11', 't 7 3 23', 't 6 2 18', 't 8 2 7', 't 7 3 24', 't 0 2 18', 'c 1 2 0', 't 1 2 2', 't 5 3 2', 't 4 1 13', 't 0 0 10', 't 5 4 1', 't 2 2 17', 't 0 2 13', 't 7 4 11', 't 8 4 17', 't 6 0 27', 't 6 1 12', 't 7 4 13', 't 8 2 10', 't 0 2 25', 't 2 3 27', 't 4 2 25', 't 3 3 24', 't 4 0 25', 't 3 1 27', 't 0 0 4', 't 3 4 17', 't 4 0 5', 't 1 4 3', 't 1 4 25', 't 3 1 12', 't 5 2 17', 't 2 3 12', 't 8 3 0', 't 0 3 0', 't 2 4 18', 't 7 1 5', 't 7 0 4', 't 7 1 19', 't 8 0 11', 't 3 3 16', 't 0 0 25', 't 2 2 27', 't 8 1 22', 't 0 1 23', 't 2 0 22', 't 4 0 11', 't 7 1 7', 't 4 2 20', 't 0 0 21', 't 1 2 4', 't 7 0 19', 't 8 0 5', 't 1 1 1', 't 5 0 4', 't 8 4 10', 't 1 0 19', 't 8 4 22', 't 1 4 18', 't 0 4 13', 't 6 4 7', 't 0 2 2', 't 6 2 11', 't 7 2 2', 't 7 2 5', 't 2 4 11', 't 2 3 16', 't 1 1 2', 't 1 0 15', 't 1 1 25', 't 5 1 5', 't 1 3 22', 't 0 3 13', 't 7 3 17', 't 2 1 27', 'c 1 1 1', 't 7 3 21', 'c 1 2 3', 'c 1 4 0', 't 7 0 1', 't 0 2 9', 't 1 3 13', 't 8 3 19', 't 7 2 27', 't 8 0 25', 't 7 1 13', 't 1 1 5', 't 6 1 21', 't 1 2 23', 't 0 0 0', 't 4 1 27', 't 3 2 23', 't 2 2 19', 't 1 2 27', 't 6 4 17', 't 4 2 23', 't 5 0 27', 't 5 2 10', 't 5 2 12', 't 8 0 21', 't 3 1 13', 't 1 1 11', 't 0 0 11', 't 7 3 9', 't 3 3 15', 't 8 3 6', 't 0 3 3', 't 7 1 3', 't 7 3 7', 't 2 0 21', 't 7 2 12', 't 2 4 3', 't 7 0 26', 't 0 1 18', 't 7 2 6', 't 4 3 18', 't 1 3 25', 't 3 0 1', 't 3 2 0', 't 6 0 2', 'c 1 3 0', 't 5 1 2', 't 2 3 6', 't 8 2 11', 't 0 3 1', 't 0 2 1', 't 8 0 13', 't 0 3 6', 't 7 3 22', 't 2 3 13', 't 2 3 24', 't 6 0 4', 't 5 2 4', 't 1 2 7', 't 8 1 9', 't 8 3 3', 't 5 2 5', 't 8 2 15', 't 6 1 25', 't 6 2 4', 't 4 2 15', 't 7 3 12', 't 2 4 1', 't 7 4 9', 't 2 2 0', 't 1 3 5', 't 4 3 1', 't 8 1 19', 't 8 3 26', 't 4 1 26', 't 0 3 16', 't 8 3 1', 't 7 0 27', 't 6 3 6', 't 2 0 7', 't 8 3 7', 't 6 2 21', 't 7 2 25', 't 8 0 19', 't 0 1 1', 't 1 4 6', 't 1 0 5', 't 8 2 3', 'c 0 4 1', 't 8 4 4', 't 0 4 22', 't 3 3 1', 't 3 0 5', 't 2 0 15', 't 2 1 18', 't 7 4 1', 't 1 1 17', 't 7 2 22', 't 6 0 17', 't 5 1 25', 't 8 0 4', 't 7 2 26', 't 6 3 15', 't 3 1 1', 't 3 3 6', 't 7 4 4', 't 3 0 13', 't 7 0 13', 't 2 4 5', 't 4 4 17', 't 0 1 4', 't 2 4 2', 't 2 3 21', 't 5 1 11', 't 6 0 13', 't 1 1 21', 't 3 0 22', 't 7 2 4', 't 3 3 22', 't 8 3 12', 't 5 4 13', 't 6 4 0', 'c 1 1 0', 't 3 0 6', 't 1 0 3', 't 8 1 25', 't 1 1 24', 't 1 1 7', 't 7 2 17', 't 5 2 20', 't 4 3 9', 't 5 3 6', 't 7 2 8', 't 2 4 16', 't 6 1 9', 't 0 0 23', 't 7 1 11', 't 6 3 17', 't 7 3 10', 't 2 1 21', 't 4 0 7', 't 4 3 13', 't 1 1 16', 't 3 0 4', 't 8 0 3', 't 6 2 8', 't 5 1 22', 't 8 1 2', 't 5 4 8', 't 0 0 2', 't 7 2 7', 't 8 2 4', 't 5 0 5', 't 2 2 18', 't 8 4 16', 't 8 0 15', 't 7 2 13', 't 3 2 26', 't 2 4 0', 't 2 0 2', 't 0 2 21', 't 8 4 5', 't 0 1 11', 't 5 1 27', 't 3 1 6', 'c 0 2 1', 't 7 0 16', 't 8 2 8', 't 1 0 4', 't 0 3 23', 't 2 4 26', 't 1 2 9', 't 4 3 8', 't 5 1 4', 't 8 2 0', 't 1 0 11', 't 0 2 22', 't 1 1 4', 't 7 3 13', 't 2 0 16', 't 4 4 20', 't 5 0 0', 't 7 0 5', 't 7 2 1', 't 7 4 27', 't 1 0 1', 't 8 0 9', 't 0 2 16', 't 7 0 9', 't 0 1 21', 't 4 2 10', 't 8 2 26', 't 4 4 11', 't 6 1 10', 't 8 1 15', 't 2 4 27', 't 8 4 0', 't 2 1 22', 't 3 0 19', 't 1 3 2', 't 8 2 24', 't 2 3 4', 't 6 2 12', 't 1 2 5', 't 7 4 3', 't 6 0 18', 't 0 3 7', 't 2 0 4', 't 2 1 26', 't 5 2 18', 't 8 3 9', 't 8 4 7', 't 4 3 15', 't 7 2 14', 't 5 0 19', 't 7 1 0', 't 8 0 1', 't 2 0 26', 't 8 0 22', 't 3 1 0', 't 1 2 3', 't 6 4 13', 't 3 0 12', 't 4 4 21', 't 0 0 6', 't 7 4 2', 't 4 1 1', 't 4 2 17', 't 6 3 24', 't 3 3 17', 't 1 1 26', 't 0 3 17', 't 5 3 16', 't 0 1 12', 't 3 2 1', 't 2 1 15', 't 5 4 7', 't 2 1 5', 't 7 3 8', 't 2 3 25', 't 6 4 6', 't 1 3 11', 't 0 4 18', 't 5 0 23', 't 4 1 5', 't 2 4 25', 't 1 4 27', 't 4 0 17', 't 1 1 12', 't 2 2 15', 't 8 1 6', 't 5 3 0', 't 1 1 22', 't 1 4 7', 't 0 2 27', 't 0 0 13', 't 0 3 26', 't 5 4 6', 't 7 1 15', 't 7 3 19', 't 1 3 12', 't 7 4 10', 't 2 4 7', 't 2 0 13', 't 0 1 7', 't 8 1 24', 'c 0 4 3', 't 8 3 5', 't 2 4 15', 't 8 3 25', 't 8 3 13', 't 5 3 22', 't 1 1 8', 't 1 0 25', 't 4 1 20', 't 7 3 6', 't 0 1 10', 't 0 1 26', 't 6 4 2', 't 0 2 7', 't 5 2 27', 't 5 1 26', 't 1 3 9', 't 7 0 24', 't 7 0 7', 't 2 3 15', 't 1 3 7', 't 7 3 27', 't 3 3 7', 't 3 4 7', 't 7 3 2', 't 8 3 15', 't 8 4 8', 't 8 1 27', 't 5 0 13', 't 1 0 26', 't 8 4 6', 't 7 2 0', 't 8 2 25', 't 7 4 8', 't 4 2 26', 't 7 3 3', 't 0 0 19', 't 5 1 24', 't 4 4 2', 't 8 4 12', 't 6 1 24', 't 6 1 17', 't 8 3 14', 't 8 2 22', 't 0 2 4', 't 6 4 3', 't 8 3 8', 't 4 4 18', 't 2 2 26', 't 0 2 12', 't 3 1 26', 'c 1 3 3', 't 7 0 8', 't 8 1 14', 't 8 2 17', 't 6 0 11', 't 1 3 14', 't 5 0 11', 't 4 3 2', 't 5 0 26', 't 7 4 21', 't 0 2 10', 't 2 2 23', 't 8 3 2', 't 8 2 1', 't 5 4 3', 't 0 3 24', 't 5 2 24', 't 5 2 1', 't 5 0 8', 't 4 2 13', 't 6 2 23', 't 8 0 18', 't 2 3 20', 't 3 4 4', 't 2 2 4', 't 5 0 24', 't 7 0 3', 't 4 3 12', 't 1 4 10', 't 8 1 7', 't 2 1 4', 't 4 4 22', 't 0 4 1', 't 1 1 9', 't 2 2 12', 't 3 4 13', 't 7 4 26', 't 6 3 13', 't 7 1 4', 't 1 4 24', 't 0 3 9', 't 0 1 6', 't 6 2 17', 't 3 1 4', 't 7 3 11', 't 8 2 16', 't 8 4 13', 't 8 4 23', 't 6 4 19', 't 4 4 10', 't 2 1 10', 't 8 4 15', 't 7 0 18', 't 6 0 26', 't 6 2 7', 't 8 2 9', 't 1 0 16', 't 4 4 0', 't 7 2 18', 't 8 3 4', 't 4 0 4', 't 7 4 20', 't 7 4 24', 't 8 3 10', 't 8 2 13', 't 2 4 19', 'c 0 1 1', 't 3 2 13', 't 3 4 16', 't 1 2 25', 't 8 4 1', 't 1 4 8', 't 7 1 20', 't 4 3 17', 't 1 0 27', 't 8 0 10', 't 7 3 0', 't 7 1 6', 't 8 1 3', 't 8 3 22', 't 6 3 16', 't 0 2 3', 't 6 0 7', 't 0 1 9', 't 1 3 26', 't 4 4 19', 't 7 0 6', 't 4 3 6', 't 1 3 15', 't 2 4 10', 't 8 2 20', 't 0 2 6', 't 5 1 18', 't 1 3 6', 't 1 1 27', 't 4 4 1', 't 7 0 22', 't 0 3 21', 't 8 1 17', 't 5 4 2', 't 0 1 3', 't 8 0 2', 't 5 1 12', 't 6 0 25', 't 4 4 3', 't 1 3 21', 't 1 2 6', 't 7 2 3', 't 7 3 26', 't 5 0 22', 't 3 2 7', 't 0 3 11', 't 7 3 16', 't 1 0 23', 'c 1 3 2', 't 4 4 9', 't 4 4 24', 't 6 1 4', 't 7 4 22', 't 5 2 6', 't 6 0 22', 't 0 4 3', 't 0 0 9', 't 1 3 8', 't 4 1 4', 't 7 4 23', 't 4 0 13', 't 8 0 23', 't 8 3 20', 't 3 0 21', 't 0 1 17', 't 0 3 12', 't 3 3 13', 't 2 4 13', 't 0 1 2', 't 1 3 17', 't 0 4 10', 't 8 1 1', 't 1 4 5', 't 3 4 6', 't 6 4 1', 't 2 0 5', 't 7 0 10', 't 2 4 17', 't 8 2 5', 't 7 4 18', 't 2 3 26', 't 7 4 15', 't 0 4 19', 't 7 3 20', 't 2 3 10', 't 6 0 24', 't 0 0 16', 't 0 0 7', 't 6 2 27', 't 5 0 17', 't 3 3 19', 't 0 4 8', 't 7 4 19', 't 2 0 27', 't 1 3 24', 't 2 4 4', 't 8 3 16', 't 7 3 18', 't 0 4 7', 't 7 4 7', 't 1 4 23', 't 1 0 0', 't 6 1 26', 't 1 2 10', 't 7 0 17', 't 7 2 9', 't 8 3 11', 't 6 1 18', 'c 0 0 1', 't 2 4 20', 't 5 3 19', 'c 1 4 3', 't 2 3 19', 't 2 0 20', 't 1 0 9', 't 1 0 6', 't 1 2 11', 't 7 4 6', 't 1 1 18', 't 1 2 21', 't 6 1 5', 't 5 2 7', 't 8 0 16', 't 1 1 3', 't 7 4 17', 't 6 2 1', 't 1 2 18', 't 5 0 6', 't 8 1 11', 't 1 4 22', 't 5 4 19', 't 0 0 15', 't 2 3 0', 't 1 3 1', 't 8 1 0', 't 1 0 13', 't 4 3 7', 't 6 3 0', 't 5 2 0', 't 3 4 22', 't 6 3 7', 't 6 0 6', 't 2 3 18', 't 8 2 6', 't 4 2 18', 't 4 3 16', 't 8 4 20', 't 7 0 20', 't 0 3 15', 't 7 2 10', 't 0 1 20', 't 8 4 25', 't 7 0 11', 't 3 2 5', 't 0 1 24', 't 7 0 21', 't 8 1 16', 't 7 0 23', 't 7 2 15', 't 0 0 3', 't 5 0 10', 't 8 1 13', 't 1 0 17', 't 8 3 17', 't 1 4 9', 't 1 3 20', 't 7 0 0', 't 0 2 23', 't 7 3 4', 't 4 0 6', 't 0 4 17', 't 3 2 21', 't 6 2 25', 't 3 2 6', 't 4 3 21', 't 8 0 14', 't 0 4 15', 't 4 2 4', 't 8 1 12', 't 2 3 1', 't 1 4 11', 't 1 4 21', 't 6 2 5', 't 1 3 3', 't 1 0 20', 't 3 1 17', 't 0 0 20', 'c 1 2 2', 't 0 4 6', 't 1 1 19', 't 0 3 8', 't 7 2 20', 't 8 1 20', 't 0 4 11', 't 4 2 0', 't 2 2 20', 't 6 1 0', 't 8 3 23', 't 2 1 20', 't 0 3 20', 't 0 3 19', 't 1 4 20', 't 8 0 20', 't 1 4 17', 't 0 4 20', 't 1 0 8', 't 1 2 20', 't 5 0 7', 't 1 2 19', 't 0 1 19', 't 0 2 20', 't 0 2 19', 't 3 4 11', 't 1 1 20', 't 1 4 19', 't 1 3 19']\nindex_set = set(index_layer_list[:440])\nprint(len(index_set))","execution_count":8,"outputs":[{"output_type":"stream","text":"440\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the time features with the filter index\n\nfrom tqdm import tqdm\ndef form_features_time(X):\n    \n    #Form array of complex numbers; convert each (2,128) sample to a (128,) sample\n    \n    n_samples = X.shape[0]\n    rows = X.shape[1]\n    vec_len = X.shape[2]\n    input_group = []\n    for ran in [0]:\n        \n        X_I_0 = X[:,0+ran,:]\n        X_Q_0 = X[:,1+ran,:]\n        X_I_list = [X_I_0]\n        X_Q_list = [X_Q_0]\n        for i in [2,4,6,8]:\n            X_I_tmp = np.concatenate((X_I_0[:, i:], X_I_0[:, :i]), axis = 1)\n            X_I_list.append(X_I_tmp * X_I_0)\n            X_Q_tmp = np.concatenate((X_Q_0[:, i:], X_Q_0[:, :i]), axis = 1)\n            X_Q_list.append(X_Q_tmp * X_Q_0)\n\n        X_complex_0 = X_I_0 + 1j*X_Q_0 # 100*1024\n        X_complex_list = [X_complex_0]   \n        for i in [2,4,6,8]:\n            X_complex_tmp = np.concatenate((X_complex_0[:, i:], X_complex_0[:, :i]), axis = 1)\n            X_complex_list.append(X_complex_tmp*X_complex_0)\n            \n        \n        X_I = np.array(X_I_list).swapaxes(0, 1)\n        X_Q = np.array(X_Q_list).swapaxes(0, 1)\n        X_complex = np.array(X_complex_list).swapaxes(0, 1)   \n        del X_I_0, X_Q_0, X_complex_0, X_I_list, X_Q_list, X_complex_list\n        \n        X_IQ_mul = X_I * X_Q\n        X_complex_abs = np.abs(X_complex) # 100 6 1024\n        X_complex_abs2 = np.abs(X_I) + np.abs(X_Q) # 100 4 1024\n        X_complex_sqr = np.square(np.abs(X_complex))  # 100 4 1024\n        X_complex_sqr2 = np.abs(np.square(X_complex))  # 100 4 1024        \n        X_complex_angl = np.angle(X_complex) # 100 4 1024\n        X_complex_angl_abs = np.abs(X_complex_angl) # 100 4 1024\n        X_complex_angl_cir = np.concatenate((X_complex_angl[:, :, 1:], X_complex_angl[:, :, X_complex_angl.shape[2]-1:]), axis = 2)\n        X_complex_fre = (X_complex_angl_cir - X_complex_angl) / 6.28 # 100 4 1024\n        X_complex_fre_abs = np.abs(X_complex_fre)\n        print(\"X_I\", X_I.shape)\n        print(\"X_Q\", X_Q.shape)\n        print(\"X_complex\", X_complex.shape)\n        input_group.extend([X_I, X_Q, X_IQ_mul, X_complex_abs, X_complex_abs2, X_complex_sqr, X_complex_sqr2, X_complex_angl, X_complex_fre])\n        \n        del X_I, X_Q, X_complex, X_complex_sqr, X_complex_abs, X_complex_angl, X_complex_angl_abs, X_complex_angl_cir, X_complex_fre, X_complex_fre_abs\n        g_num = len(input_group)\n\n        \n    dict_features = {}\n    for i in tqdm(range(len(input_group))):\n        for j in range(input_group[0].shape[1]):\n            # 9 * 5 * 28 = 1260\n            index_list = []\n            for z in range(28):\n                index = 't' + ' ' + str(i) + ' ' + str(j) + ' ' + str(z)\n                if index in index_set:\n                    index_list.append(z)\n#                 index_list.append(z)\n            for k in index_list:\n                if k==0:\n                    dict_features['element_mean'+str(i)+str(j)] = element_mean(input_group[i][:, j:j+1, :])\n                elif k==1:\n                    dict_features['element_abs_mean'+str(i)+str(j)] = element_abs_mean(input_group[i][:, j:j+1, :])\n                elif k==2:\n                    dict_features['element_std'+str(i)+str(j)] = element_std(input_group[i][:, j:j+1, :])\n                elif k==3:\n                    dict_features['element_std_abs'+str(i)+str(j)] = element_std_abs(input_group[i][:, j:j+1, :])\n                elif k==4:\n                    dict_features['abs_max_mins_min'+str(i)+str(j)] = abs_max_mins_min(input_group[i][:, j:j+1, :])\n                elif k==5:\n                    dict_features['max_mins_min_abs'+str(i)+str(j)] = max_mins_min_abs(input_group[i][:, j:j+1, :])\n                elif k==6:\n                    dict_features['abs_mean_sqr'+str(i)+str(j)] = abs_mean_sqr(input_group[i][:, j:j+1, :])\n                elif k==7:\n                    dict_features['mean_sqr_abs'+str(i)+str(j)] = mean_sqr_abs(input_group[i][:, j:j+1, :])\n                elif k==8:\n                    dict_features['abs_sqrmean_mis_meansqr'+str(i)+str(j)] = abs_sqrmean_mis_meansqr(input_group[i][:, j:j+1, :])\n                elif k==9:\n                    dict_features['sqrmean_mis_meansqr_abs'+str(i)+str(j)] = sqrmean_mis_meansqr_abs(input_group[i][:, j:j+1, :])\n                elif k==10:\n                    dict_features['abs_fourmean_mis_meanfour'+str(i)+str(j)] = abs_fourmean_mis_meanfour(input_group[i][:, j:j+1, :])\n                elif k==11:\n                    dict_features['fourmean_mis_meanfour_abs'+str(i)+str(j)] = fourmean_mis_meanfour_abs(input_group[i][:, j:j+1, :])\n                \n\n                # part two\n                elif k==12:\n                    dict_features['absolute_sum_of_changes'+str(i)+str(j)] = absolute_sum_of_changes(input_group[i][:, j:j+1, :])\n                elif k==13:\n                    dict_features['approximate_entropy'+str(i)+str(j)] = approximate_entropy(input_group[i][:, j:j+1, :])\n                elif k==14:\n                    dict_features['autocorrelation'+str(i)+str(j)] = autocorrelation(input_group[i][:, j:j+1, :])\n                elif k==15:\n                    dict_features['count_above_mean'+str(i)+str(j)] = count_above_mean(input_group[i][:, j:j+1, :])\n                elif k==16:\n                    dict_features['count_below_mean'+str(i)+str(j)] = count_below_mean(input_group[i][:, j:j+1, :])\n                elif k==17:\n                    dict_features['cwt_coefficients'+str(i)+str(j)] = cwt_coefficients(input_group[i][:, j:j+1, :])\n\n                elif k==18:\n                    dict_features['kurtosis'+str(i)+str(j)] = kurtosis(input_group[i][:, j:j+1, :])\n                elif k==19:\n                    dict_features['skewness'+str(i)+str(j)] = skewness(input_group[i][:, j:j+1, :])\n\n                # part three\n                elif k==20:\n                    dict_features['max_amp_density_abs'+str(i)+str(j)] = max_amp_density_abs(input_group[i][:, j:j+1, :])\n                elif k==21:\n                    dict_features['std_phase_abs'+str(i)+str(j)] = std_phase_abs(input_group[i][:, j:j+1, :], \n                                                                                 input_group[int(i/g_num)*g_num+2][:, j:j+1, :])\n                elif k==22:\n                    dict_features['std_phase'+str(i)+str(j)] = std_phase(input_group[i][:, j:j+1, :], \n                                                                         input_group[int(i/g_num)*g_num+2][:, j:j+1, :])\n                elif k==23:\n                    dict_features['std_amp_abs'+str(i)+str(j)] = std_amp_abs(input_group[i][:, j:j+1, :])\n                elif k==24:\n                    dict_features['std_fre_abs'+str(i)+str(j)] = std_fre_abs(input_group[i][:, j:j+1, :], \n                                                                             input_group[int(i/g_num)*g_num+2][:, j:j+1, :])\n                elif k==25:\n                    dict_features['std_amp'+str(i)+str(j)] = std_amp(input_group[i][:, j:j+1, :])\n                elif k==26:\n                    dict_features['den_amp'+str(i)+str(j)] = den_amp(input_group[i][:, j:j+1, :])\n                elif k==27:\n                    dict_features['den_fre'+str(i)+str(j)] = den_fre(input_group[i][:, j:j+1, :])\n            \n            \n    #Concatenate feature arrays\n    features = []\n    features += dict_features.values()\n    features = np.concatenate(np.array(features), axis = 2)\n   \n    return features.reshape(features.shape[0], -1)\n\nX_train_feature_time = form_features_time(X_train[:,:,:])\nX_test_feature_time = form_features_time(X_test[:,:,:])\nprint(\"X_train_feature_time\", X_train_feature_time.shape)\nprint(\"X_test_feature_time\", X_test_feature_time.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/9 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"X_I (30000, 5, 1024)\nX_Q (30000, 5, 1024)\nX_complex (30000, 5, 1024)\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 9/9 [02:04<00:00, 13.85s/it]\n  0%|          | 0/9 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"X_I (20000, 5, 1024)\nX_Q (20000, 5, 1024)\nX_complex (20000, 5, 1024)\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 9/9 [01:23<00:00,  9.25s/it]","name":"stderr"},{"output_type":"stream","text":"X_train_feature_time (30000, 405)\nX_test_feature_time (20000, 405)\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the complex features with the filter index \n\nfrom tqdm import tqdm\ndef form_features_complex(X):\n    \n    #Form array of complex numbers; convert each (2,128) sample to a (128,) sample\n    \n    n_samples = X.shape[0]\n    rows = X.shape[1]\n    vec_len = X.shape[2]\n        \n    X_complex_0_1 = X[:,0,:] + 1j*X[:,1,:] # 100*1024\n    X_complex_0_2 = X[:,2,:] + 1j*X[:,3,:] # 100*1024\n\n    phase_list = [X_complex_0_1, X_complex_0_2]\n    del X_complex_0_1, X_complex_0_2\n    \n    input_group = []\n    for j in range(len(phase_list)):\n        X_complex_list = [phase_list[j]]\n        for i in [2,4,6,8]:\n            X_complex_tmp = np.concatenate((phase_list[j][:, i:], phase_list[j][:, :i]), axis = 1)\n            X_complex_list.append(X_complex_tmp*phase_list[j])\n        X_complex_arr = np.array(X_complex_list).swapaxes(0, 1)\n        input_group.append(X_complex_arr)\n\n    print(\"input_group_len\", len(input_group))\n    print(\"input_group[0]_shape\", input_group[0].shape)        \n    dict_features = {}\n    \n    for i in tqdm(range(len(input_group))):\n        for j in range(input_group[0].shape[1]):\n            # 2 * 5 * 6 = 60\n            index_list = []\n            for z in range(6):\n                index = 'c' + ' ' + str(i) + ' ' + str(j) + ' ' + str(z)\n                if index in index_set:\n                    index_list.append(z)\n#                 index_list.append(z)\n            for k in index_list:\n                if k == 0:\n                    dict_features['element_abs_mean'+str(i)+str(j)] = element_abs_mean(input_group[i][:, j:j+1, :])\n                elif k == 1:   \n                    dict_features['abs_max_mins_min'+str(i)+str(j)] = abs_max_mins_min(input_group[i][:, j:j+1, :])\n                elif k == 2:    \n                    dict_features['abs_mean_sqr'+str(i)+str(j)] = abs_mean_sqr(input_group[i][:, j:j+1, :])\n                elif k == 3:     \n                    dict_features['abs_sqrmean_mis_meansqr'+str(i)+str(j)] = abs_sqrmean_mis_meansqr(input_group[i][:, j:j+1, :])\n                elif k == 4:     \n                    dict_features['abs_fourmean_mis_meanfour'+str(i)+str(j)] = abs_fourmean_mis_meanfour(input_group[i][:, j:j+1, :])\n                elif k == 5:     \n                    dict_features['absolute_sum_of_changes'+str(i)+str(j)] = absolute_sum_of_changes(input_group[i][:, j:j+1, :])\n\n        \n    #Concatenate feature arrays\n    features = []\n    features += dict_features.values()\n    features = np.concatenate(np.array(features), axis = 2)\n    return features.reshape(features.shape[0], -1)\n\nX_train_feature_complex = form_features_complex(X_train)\nX_test_feature_complex = form_features_complex(X_test)\nprint(X_train_feature_complex.shape)\nprint(X_test_feature_complex.shape)","execution_count":10,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/2 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"input_group_len 2\ninput_group[0]_shape (30000, 5, 1024)\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 2/2 [00:10<00:00,  5.05s/it]\n  0%|          | 0/2 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"input_group_len 2\ninput_group[0]_shape (20000, 5, 1024)\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 2/2 [00:06<00:00,  3.39s/it]","name":"stderr"},{"output_type":"stream","text":"(30000, 35)\n(20000, 35)\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concate time feature and complex feature together\nX_train_feature = np.concatenate((X_train_feature_time, X_train_feature_complex), axis = 1)\nX_test_feature = np.concatenate((X_test_feature_time, X_test_feature_complex), axis = 1)\n\nprint(X_train_feature.shape)\nprint(X_test_feature.shape)","execution_count":12,"outputs":[{"output_type":"stream","text":"(30000, 440)\n(20000, 440)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize the features\n\nfrom sklearn.feature_selection import VarianceThreshold\nX_all = np.concatenate((X_train_feature, X_test_feature), axis = 0)\n\ns0 = X_train_feature.shape[0]\ns1 = X_train_feature.shape[1]\n\nk = 1\ninterval = int(s1/k)\nX_train_std_all_list = []\nX_test_std_list = []\nsc_train = []\nfor i in range(k):\n    X_all_filter = VarianceThreshold(threshold=(0)).fit_transform(X_all[:,interval*i: interval*(i+1)])\n    sc_train.append(StandardScaler())\n    print(X_all_filter.shape)\n    sc_train[i].fit(X_all_filter)\n    X_train_std_all_list.append(sc_train[i].transform(X_all_filter[:s0,:])) \n    X_test_std_list.append(sc_train[i].transform(X_all_filter[s0:,:]))\nX_train_std_all = np.concatenate((X_train_std_all_list), axis = 1)\nX_test_std = np.concatenate((X_test_std_list), axis = 1)\n\nprint(\"X_train_std_all,\", X_train_std_all.shape)\nprint(\"X_test_std,\", X_test_std.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"(50000, 440)\nX_train_std_all, (30000, 440)\nX_test_std, (20000, 440)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# devide train and val data\n\nX_train_std, X_val_std, y_train, y_val = train_test_split(X_train_std_all, y_train_all, test_size=1/5, random_state=42)\nprint(\"X_train_std,\", X_train_std.shape)\nprint(\"X_val_std,\", X_val_std.shape)\nprint(\"X_test_std,\", X_test_std.shape)","execution_count":14,"outputs":[{"output_type":"stream","text":"X_train_std, (24000, 440)\nX_val_std, (6000, 440)\nX_test_std, (20000, 440)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train basic model: SVC model -- the best basic model\n\nfrom sklearn.svm import SVC\n\naccuracy_tst = []\naccuracy_trn = []\np_train_svm_gau = np.zeros(X_train_std.shape)\np_val_svm_gau = np.zeros(X_val_std.shape)\np_test_svm_gau = np.zeros(X_test_std.shape)\n# indices = 2**np.array(range(2,8))\nindices = [30]\n\nfor i in range(len(indices)):\n    print(indices[i])\n    svm_gau = SVC(kernel='rbf', C = indices[i])\n    svm_gau.fit(X_train_std, y_train)\n    p_train_svm_gau = svm_gau.predict(X_train_std)\n    p_val_svm_gau = svm_gau.predict(X_val_std)\n    p_test_svm_gau = svm_gau.predict(X_test_std)\n    accuracy_trn.append(svm_gau.score(X_train_std, y_train))\n    accuracy_tst.append(svm_gau.score(X_val_std, y_val))\n    print(\"Acc_Train[i],\", accuracy_trn[i])\n    print(\"Acc_Test[i],\", accuracy_tst[i])","execution_count":20,"outputs":[{"output_type":"stream","text":"30\nAcc_Train[i], 0.7625416666666667\nAcc_Test[i], 0.5981666666666666\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # train basic model: navie gaussian model\n\n# from sklearn.naive_bayes import GaussianNB\n\n# gnb = GaussianNB()\n# gnb.fit(X_train_std, y_train)\n# p_train_naive_gau = gnb.predict(X_train_std)\n# p_val_naive_gau = gnb.predict(X_val_std)\n# p_test_naive_gau = gnb.predict(X_test_std)\n# score_train = gnb.score(X_train_std, y_train)\n# score_val = gnb.score(X_val_std, y_val)\n# print(score_train)","execution_count":17,"outputs":[{"output_type":"stream","text":"0.4700416666666667\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # train basic model: GradientBoosting model\n\n# from sklearn.datasets import make_classification\n# from sklearn.ensemble import GradientBoostingClassifier\n\n# clf = GradientBoostingClassifier(random_state=0)\n# clf.fit(X_train_std, y_train)\n# score = clf.score(X_val_std, y_val)\n# print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # stack the result of the basic models\n\n# train_second = np.stack((p_train_naive_gau, p_train_svm_gau), 1)\n# val_second = np.stack((p_val_naive_gau, p_val_svm_gau), 1)\n# test_second = np.stack((p_test_naive_gau, p_test_svm_gau), 1)\n\n# print(train_second.shape)\n# print(val_second.shape)\n# print(test_second.shape)","execution_count":31,"outputs":[{"output_type":"stream","text":"(24000, 2)\n(6000, 2)\n(20000, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # transfer results of basic models into loader\n\n# import torch\n# from torch.autograd import Variable\n# from torchvision import transforms\n# from torch.utils.data import Dataset, DataLoader\n# from PIL import Image\n\n# transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n# class MyDataset(Dataset):\n#     def __init__(self, data, label=None, transform=None):\n#         datas = []\n#         for i in range(data.shape[0]):\n#             datas.append((data[i], int(label[i])))\n#         self.datas = datas\n#         self.transform = transform\n\n#     def __getitem__(self, index):\n#         fn, label = self.datas[index]\n#         data = torch.from_numpy(fn)\n#         return data,label\n\n#     def __len__(self):\n#         return len(self.datas)\n\n# train = MyDataset(data = np.float32(train_second), label = y_train)\n# trainloader = DataLoader(dataset=train, batch_size=32, shuffle=True)\n\n# val = MyDataset(data = np.float32(val_second), label = y_val)\n# valloader = DataLoader(dataset=val, batch_size=64, shuffle=True)\n\n# y_test = np.zeros(X_test.shape[0])\n# test = MyDataset(data = np.float32(test_second), label = y_test)\n# testloader = DataLoader(dataset=test, batch_size=32)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # use NN to stack the results of the basic models\n\n# import torch\n# import torch.nn as nn\n# from torch.autograd import Variable\n# import torch.optim as optim\n# import torch.nn.functional as F\n# import torch.utils.data as data\n# import torchvision\n# import torchvision.transforms as transforms\n# import torchvision.datasets as vdatasets\n# import torchvision.utils as vutils\n\n# #Define Neural Network class here \n# class FeedFrwdNet(nn.Module):\n#     def __init__(self):\n#         super(FeedFrwdNet,self).__init__()\n#         self.dense1 = nn.Sequential(\n#             nn.Linear(2, 32),\n#             nn.ReLU()\n#         )\n#         self.dense2 = nn.Sequential(\n#             nn.Linear(32, 32),\n#             nn.ReLU()\n#         )\n#         self.dense3 = nn.Sequential(\n#             nn.Linear(32, 10),\n#         )\n\n#     def forward(self,input):\n#         x1 = self.dense1(input)\n#         x2 = self.dense2(x1)\n#         x3 = self.dense3(x2)\n#         return x3\n\n# def test(loader, kind):\n#     correct = 0\n#     total = 0\n#     for data in loader:\n#         inputs, labels = data\n#         model.eval()\n#         outputs = model(inputs)\n#         _, predicted = torch.max(outputs.data, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum()\n#     accuracy = 100.0 * correct / total\n#     print(\"acc of \" + kind + \" is \"+ str(accuracy))\n\n\n\n# model = FeedFrwdNet()\n# loss = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99), eps=1e-08, weight_decay=0.001) \n\n# #Training function \n# def train(epoch, log_interval=100):\n#     for batch_idx, data in enumerate(trainloader2,0):\n#         inputs, labels = data\n#         optimizer.zero_grad()\n#         model.train()\n#         outputs = model(inputs)\n#         loss_v = loss(outputs, labels)\n#         loss_v.backward()\n#         optimizer.step()\n\n\n# epochs= 50\n# model_save = FeedFrwdNet()\n\n# for epoch in range(1, epochs + 1):\n#         print(epoch)\n#         train(epoch)\n#         test(trainloader, 'train')\n#         test(valloader, 'val')\n","execution_count":33,"outputs":[{"output_type":"stream","text":"1\nacc of train is tensor(56.4875)\nacc of val is tensor(45.8833)\n2\nacc of train is tensor(56.9750)\nacc of val is tensor(44.6667)\n3\nacc of train is tensor(73.0208)\nacc of val is tensor(58.2333)\n4\nacc of train is tensor(74.3250)\nacc of val is tensor(58.9667)\n5\nacc of train is tensor(71.8125)\nacc of val is tensor(56.3500)\n6\nacc of train is tensor(73.2708)\nacc of val is tensor(58.4833)\n7\nacc of train is tensor(74.4875)\nacc of val is tensor(59.0167)\n8\nacc of train is tensor(74.7250)\nacc of val is tensor(59.1167)\n9\nacc of train is tensor(74.7958)\nacc of val is tensor(59.2000)\n10\nacc of train is tensor(74.9208)\nacc of val is tensor(59.1667)\n11\nacc of train is tensor(74.8000)\nacc of val is tensor(59.1000)\n12\nacc of train is tensor(74.5042)\nacc of val is tensor(59.0333)\n13\nacc of train is tensor(75.0417)\nacc of val is tensor(59.1833)\n14\nacc of train is tensor(75.0292)\nacc of val is tensor(59.1833)\n15\nacc of train is tensor(74.9208)\nacc of val is tensor(59.1667)\n16\nacc of train is tensor(74.9250)\nacc of val is tensor(59.1667)\n17\nacc of train is tensor(75.1625)\nacc of val is tensor(59.2500)\n18\nacc of train is tensor(74.9208)\nacc of val is tensor(59.1667)\n19\nacc of train is tensor(74.9208)\nacc of val is tensor(59.1667)\n20\nacc of train is tensor(74.6167)\nacc of val is tensor(59.0833)\n21\nacc of train is tensor(76.0125)\nacc of val is tensor(59.7833)\n22\nacc of train is tensor(74.9208)\nacc of val is tensor(59.1667)\n23\nacc of train is tensor(75.8042)\nacc of val is tensor(59.6667)\n24\nacc of train is tensor(74.6167)\nacc of val is tensor(59.0833)\n25\nacc of train is tensor(74.6167)\nacc of val is tensor(59.0833)\n26\nacc of train is tensor(76.0125)\nacc of val is tensor(59.7833)\n27\nacc of train is tensor(76.1375)\nacc of val is tensor(59.8500)\n28\nacc of train is tensor(75.8042)\nacc of val is tensor(59.6667)\n29\nacc of train is tensor(76.1750)\nacc of val is tensor(59.8500)\n30\nacc of train is tensor(76.1792)\nacc of val is tensor(59.8500)\n31\nacc of train is tensor(76.1875)\nacc of val is tensor(59.8500)\n32\nacc of train is tensor(75.5500)\nacc of val is tensor(59.5833)\n33\nacc of train is tensor(76.1958)\nacc of val is tensor(59.8500)\n34\nacc of train is tensor(75.8542)\nacc of val is tensor(59.6667)\n35\nacc of train is tensor(76.1375)\nacc of val is tensor(59.8500)\n36\nacc of train is tensor(76.1917)\nacc of val is tensor(59.8500)\n37\nacc of train is tensor(76.2167)\nacc of val is tensor(59.8500)\n38\nacc of train is tensor(76.1792)\nacc of val is tensor(59.8500)\n39\nacc of train is tensor(75.8542)\nacc of val is tensor(59.6667)\n40\nacc of train is tensor(76.2083)\nacc of val is tensor(59.8167)\n41\nacc of train is tensor(76.2125)\nacc of val is tensor(59.8500)\n42\nacc of train is tensor(76.2125)\nacc of val is tensor(59.8500)\n43\nacc of train is tensor(76.2125)\nacc of val is tensor(59.8500)\n44\nacc of train is tensor(76.2125)\nacc of val is tensor(59.8500)\n45\nacc of train is tensor(76.2167)\nacc of val is tensor(59.8500)\n46\nacc of train is tensor(76.2500)\nacc of val is tensor(59.8167)\n47\nacc of train is tensor(76.1958)\nacc of val is tensor(59.8500)\n48\nacc of train is tensor(76.2167)\nacc of val is tensor(59.8500)\n49\nacc of train is tensor(76.2333)\nacc of val is tensor(59.8500)\n50\nacc of train is tensor(76.2333)\nacc of val is tensor(59.8500)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ncon_max = confusion_matrix(y_val, p_val_svm_gau)\nprint(con_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train random forest to rank the feature and get the filter index which can be used in the former part\n# after get the filter index, this part can be not used if no new feature is introduced\n\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.datasets import make_classification\n# Acc_Train = np.zeros(20)\n# Acc_Test = np.zeros(20)\n# p_train_forest = np.zeros(X_train_std.shape)\n# p_val_forest = np.zeros(X_val_std.shape)\n# indices = 2**np.array(range(2,10))\n# feature_importances = []\n# # indices = range(10, 20)\n\n# for i in range(len(indices)):\n\n#     forest = RandomForestClassifier(max_depth=15, n_estimators=indices[i], random_state=0)\n#     forest.fit(X_train_std, y_train)\n#     p_val_forest = forest.predict(X_val_std)\n#     p_train_forest = forest.predict(X_train_std)\n#     Acc_Train[i] = forest.score(X_train_std, y_train)\n#     Acc_Test[i] = forest.score(X_val_std, y_val)\n#     feature_importances.append(np.array(forest.feature_importances_))\n#     print(i)\n#     print(\"Acc_Train[i],\", Acc_Train[i])\n#     print(\"Acc_Test[i],\", Acc_Test[i])    \n# # sorted_idx = np.argsort(-feature_importances)\n\n\n# sorted_idx = np.argsort(-feature_importances[3])\n# time_len = X_train_feature_time.shape[1]\n# comp_len = X_train_feature_complex.shape[1]\n# index_layer = []\n# times = [9,5,28]\n# complexs = [2,5,6]\n# for i in range(len(sorted_idx)):\n#     index = sorted_idx[i]\n#     if index < time_len:\n#         ty = 't'\n#         sub1 = int(index / (times[1]*times[2]))\n#         sub2 = int((index % (times[1]*times[2]))/times[2])\n#         sub3 = int((index % (times[1]*times[2]))%times[2])\n#         index_layer.append(ty + ' ' + str(sub1) + ' ' + str(sub2) + ' ' + str(sub3))\n#     else:\n#         ty = 'c'\n#         index -= time_len\n#         sub1 = int(index / (complexs[1]*complexs[2]))\n#         sub2 = int((index % (complexs[1]*complexs[2]))/complexs[2])\n#         sub3 = int((index % (complexs[1]*complexs[2]))%complexs[2])\n#         index_layer.append(ty + ' ' + str(sub1) + ' ' + str(sub2) + ' ' + str(sub3))\n\n# # for i in range(500):\n# #     print(index_layer[i], ' ',feature_importances[-1][sorted_idx[i]])\n# print(index_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the prediction result of test dataset\n\ntest_pre = p_test_svm_gau\nrows_write = []\nname_dic = { 0:'FM', 1:'OQPSK', 2:'BPSK', 3:'8PSK', 4:'AM-SSB-SC', 5:'4ASK', 6:'16PSK', 7:'AM-DSB-SC', 8:'QPSK', 9:'OOK' }\nfor i in range(len(test_pre)):\n    rows_write.append(name_dic[test_pre[i]])\n\nsubmit_df = pd.DataFrame({\"Id\": range(len(test_pre)), \"Category\": rows_write})\nsubmit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}